{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Astronomy 8824 - Numerical and Statistical Methods in Astrophysics\n",
    "\n",
    "## Statistical Methods Topic VI. Estimating Errors From Data\n",
    "\n",
    "These notes are for the course Astronomy 8824: Numerical and Statistical Methods in Astrophysics. It is based on notes from David Weinberg with modifications and additions by Paul Martini.\n",
    "David's original notes are available from his website: http://www.astronomy.ohio-state.edu/~dhw/A8824/index.html\n",
    "\n",
    "#### Background reading: \n",
    "- Statistics, Data Mining, and Machine Learning in Astronomy, $\\S 4.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy.polynomial import Polynomial\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy import stats\n",
    "\n",
    "# matplotlib settings \n",
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 16\n",
    "BIGGER_SIZE = 18\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('lines', linewidth=2)\n",
    "plt.rc('axes', linewidth=2)\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)   # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LaTex macros hidden here -- \n",
    "$\\newcommand{\\expect}[1]{{\\left\\langle #1 \\right\\rangle}}$\n",
    "$\\newcommand{\\intinf}{\\int_{-\\infty}^{\\infty}}$\n",
    "$\\newcommand{\\xbar}{\\overline{x}}$\n",
    "$\\newcommand{\\ybar}{\\overline{y}}$\n",
    "$\\newcommand{\\like}{{\\cal L}}$\n",
    "$\\newcommand{\\llike}{{\\rm ln}{\\cal L}}$\n",
    "$\\newcommand{\\xhat}{\\hat{x}}$\n",
    "$\\newcommand{\\yhat}{\\hat{y}}$\n",
    "$\\newcommand{\\xhati}{\\hat{x}_i}$\n",
    "$\\newcommand{\\yhati}{\\hat{y}_i}$\n",
    "$\\newcommand{\\sigxi}{\\sigma_{x,i}}$\n",
    "$\\newcommand{\\sigyi}{\\sigma_{y,i}}$\n",
    "$\\newcommand{\\cij}{C_{ij}}$\n",
    "$\\newcommand{\\cinvij}{C^{-1}_{ij}}$\n",
    "$\\newcommand{\\cinvkl}{C^{-1}_{kl}}$\n",
    "$\\newcommand{\\cinvmn}{C^{-1}_{mn}}$\n",
    "$\\newcommand{\\valpha}{\\vec \\alpha}$\n",
    "$\\newcommand{\\vth}{\\vec \\theta}$\n",
    "$\\newcommand{\\ymod}{y_{\\rm mod}}$\n",
    "$\\newcommand{\\dy}{\\Delta y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where does the error bar go? \n",
    "\n",
    "Suppose you measure the average depression of flux in a quasar caused by absorption from the Lyman-alpha forest.  You find that 30\\% of the flux is absorbed, $D_A=0.3$.\n",
    "\n",
    "You have two models that predict $D_A=0.32$ and $D_A=0.4$, respectively. Which do the data favor?  Is either ruled out?\n",
    "\n",
    "To answer, we need an error bar, and this may be different for the two models.  \n",
    "\n",
    "If the first model predicts $D_A=0.32$ on average and an rms variation of $0.002$ from one quasar to another, then the predicted $D_A=0.32 \\pm 0.002$ is strongly inconsistent with the observed $D_A=0.3$, unless the predicted distribution of variations is highly non-Gaussian.\n",
    "\n",
    "If the second model predicts $D_A=0.4$ on average and an rms variation of 0.05 from one quasar to another, then its prediction $D_A=0.4 \\pm 0.05$ is marginally inconsistent with your measurement.  \n",
    "The data favor this model even though its mean prediction is further from the observed value.\n",
    "\n",
    "\n",
    "This example illustrates the Bayesian insistence that error bars really belong _on the model_, not on the data, since different models may predict different error bars for the same data set.\n",
    "\n",
    "But suppose we measure the decrement for 20 quasars instead of one, and we find a mean of 0.3 and an rms variation about the mean of 0.05.  \n",
    "\n",
    "Here it seems legitimate to say that the uncertainty on the mean is $0.05/\\sqrt{20}=0.01$,\n",
    "and that our measurement implies $D_A=0.3 \\pm 0.01$.\n",
    "\n",
    "What allows us to attach an error bar to the data, and to implicitly claim that it is model independent?  \n",
    "\n",
    "In effect, this procedure relies on the assumption (which should be good in this case) that any model that will fit the data must also predict an rms variation similar to the value $0.05$ that you measured, and that it will therefore predict an error on the mean for a sample of 20 quasars that is close to $0.05/\\sqrt{20}$.\n",
    "\n",
    "A model that predicts a mean $D_A=0.35$ and an rms quasar-to-quasar variation of $0.3$ gives $D_A=0.35 \\pm 0.06$ for a sample of 20 quasars.  But although its mean prediction is consistent with the measured mean within its expected error, the rms variation for this model is inconsistent with the measured rms variation of 0.05, so the model is ruled out, or at least disfavored, on other grounds. (To decide just how inconsistent the model is, we would need to\n",
    "calculate the error bar on the rms variation.)\n",
    "\n",
    "\n",
    "For quantities that are well measured (i.e., determined to fairly high fractional precision), it is usually OK to \"transfer\" the error bar in this way, because the data have sufficient power to constrain the variation within the sample and yield an estimated error bar that must be close to that of any model that would be\n",
    "consistent with the data.\n",
    "\n",
    "However, you should be very cautious about \"transferring\" the error bar in any case where the estimated fractional uncertainty is large. In these cases, the error bar is often highly model dependent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Number Statistics\n",
    "\n",
    "The extreme, and often relevant, example is a survey that turns up one object of a certain class.  \n",
    "\n",
    "It is tempting to say that the measured number of objects is $1 \\pm 1$ and therefore consistent with zero.\n",
    "\n",
    "It is true that a model that predicts a mean of one object in a survey of this size predicts (assuming Poisson statistics) that a fraction $e^{-1} = 0.37$ of such surveys would detect no objects, and that a model that predicts a mean of two objects predicts that $2^1 e^{-2}/1! = 0.27$ of such surveys should yield one object and is therefore consistent with the data.\n",
    "\n",
    "However, if we have a model that predicts a mean of 0.001 objects, then it predicts that only $0.001^1 e^{-0.001}/1! = 0.001$ of such surveys should yield one object, so it is ruled out (or at least strongly disfavored).\n",
    "\n",
    "A model that predicts a mean of 10 objects is also strongly disfavored, as $P(k=1|\\mu=10) = 10^1 e^{-10} = 4.5 \\times 10^{-4}$.\n",
    "\n",
    "Even with very small numbers of objects, one can make statistically interesting statements about some models.\n",
    "\n",
    "#### Low-background example\n",
    "\n",
    "This example is relevant to data analysis with very low backgrounds (e.g. _Chandra_). \n",
    "\n",
    "Suppose that the background is very low, e.g., $10^{-4}$ counts/pixel in a 50 ksec exposure.\n",
    "\n",
    "If you have $10^6$ pixels, there will be $\\sim 100$ background counts, so a single-photon detection isn't significant.\n",
    "\n",
    "However, the probability of getting 2 background photons in a pixel is $10^{-8}$, so there should be only $0.01$ pixels out of $10^6$ with two background photons by chance.\n",
    "\n",
    "Therefore, 2 counts in a single pixel would be a statistically significant detection of an object.\n",
    "\n",
    "Also, if you knew ahead of time where you were going to look (e.g., at a recent TDE) to within a pixel, then even a single photon detection would be significant at the $10^{-4}$ level, and would rule out a model that predicted only $10^{-3}$ source counts in a 50 ksec exposure.\n",
    "\n",
    "$$\n",
    "P(k=1) = \\frac{0.001^1 \\exp^{-0.001}}{1!} = 0.001\n",
    "$$\n",
    "In words, the probability of one count when $10^{-3}$ are expected is $P(k=1) = 0.001$.\n",
    "\n",
    "**Moral:** Don't automatically discount small number statistics, though you should use them with caution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating error bars from the data: subsample, jackknife, bootstrap\n",
    "\n",
    "In a case like the average quasar flux decrement above, it is obvious how to estimate the error bar from the data using the rms variation.\n",
    "\n",
    "But suppose we are doing something more complicated, e.g., measuring the power spectrum of the flux (a 1-d function) after fitting a  continuum to each spectrum, removing metal lines, and subtracting photon noise.  We have done some complicated processing, and the signal-to-noise of the measurement in each individual spectrum may be quite different from one quasar to another.\n",
    "\n",
    "It may not be obvious to to estimate an error bar from the data using the rms variation. It is not always correct to assume Gaussianity. \n",
    "\n",
    "\n",
    "#### Subsample\n",
    "\n",
    "One way to proceed in a complicated case like this is to divide the data into subsamples, say five groups of four quasars each. You can then apply your measurement separately to each subsample and estimate errors from the subsample-to-subsample variation.\n",
    "\n",
    "For example, you now have $N=5$ estimates $^kP_i$ of the power spectrum on spatial scale $i$, where $k=1,...N$.  You can estimate the error bar $\\sigma_{ii}$ on $P_i$ as\n",
    "$$\n",
    "\\sigma^2_{ii} = {1 \\over N-1} \\sum_{k=1}^N {(^kP_i - \\bar{P_i})^2 \\over N},\n",
    "$$\n",
    "where $\\bar{P_i}$ is your estimate from the full sample of all quasars.\n",
    "\n",
    "You might also want to estimate the covariance of errors from two different length scales $i$ and $j$:\n",
    "$$\n",
    "\\sigma^2_{ij} = {1 \\over N-1} \\sum_{k=1}^N \n",
    "  {(^kP_i - \\bar{P_i})(^kP_j-\\bar{P_j}) \\over N}.\n",
    "$$\n",
    "\n",
    "(Some covariance is expected due to the correlated errors in the measurement of the power spectrum.)\n",
    "\n",
    "#### Jackknife\n",
    "\n",
    "The subsample approach can lead to problems if you really need something close to your full sample size to get a usable measurement in the first place, so that the estimates from your much smaller subsamples are  wildly varying.  (This is especially problematic if, for instance, you know that the quantity you are measuring is positive-definite\n",
    "but noise means that you can get negative values in individual measurements.)\n",
    "\n",
    "An attractive, more robust alternative is jackknife error estimation, where you _omit_ each subsample in turn, and apply your measurement to all of the _remaining_ data.  \n",
    "\n",
    "For example, imagine you want to measure the parameter $\\alpha$, and $\\alpha^N$ is the value of $\\alpha$ if measured from all $N$ data points. We can create $N$ datasets that each are missing one measurement $i$ called $\\alpha^*_i$. The bias-corrected jackknife estimate of $\\alpha$ is:\n",
    "$$\n",
    "\\alpha^J = \\alpha_N + \\Delta \\alpha = \\alpha_N + (N-1) \\left( \\alpha_N - \\frac{1}{N} \\sum^N \\alpha^*_i \\right)\n",
    "$$\n",
    "\n",
    "The error estimate in this case is\n",
    "$$\n",
    "\\sigma^2_{ii} = {N-1 \\over N} \\sum_{k=1}^N (^kP_i - \\bar{P_i})^2 ,\n",
    "$$\n",
    "where $^kP_i$ now represents the estimate of $P_i$ after subsample $k$ is _omitted_ from the data sample.\n",
    "\n",
    "The pre-factor is larger by a factor of $(N-1)^2$, but the variation $(^kP_i - \\bar{P_i})$ is smaller because each subset $k$ is now close to the full sample.\n",
    "\n",
    "In the case we are considering, the individual subsamples could now be single quasars, so we could set $N=20$ and omit each quasar in turn.\n",
    "\n",
    "Where the jackknife and subsample error estimates would give different answers, I think the jackknife estimate is generally preferable.\n",
    "\n",
    "#### Bootstrap\n",
    "\n",
    "A widely used variant on the same theme is bootstrap resampling. Here you create new samples the same size as the original data sample by drawing from that sample \"with replacement.\"\n",
    "\n",
    "For example, $N$ bootstrap samples has $M=20$ quasars randomly drawn from the  original set, but in an individual sample quasar 1 may appear three times, quasar 2 twice, and quasar 3 not at all.\n",
    "\n",
    "The error bars are simply computed from the dispersion among the $M$ bootstrap samples,\n",
    "$$\n",
    "\\sigma^2_{ii} = \\sum_{k=1}^N {(^kP_i - \\bar{P_i})^2 \\over N},\n",
    "$$\n",
    "where $\\bar{P_i}$ is the estimate from the full sample, not the mean of the bootstrap samples, and $^kP_i$ are the bootstrap samples.  There is no pre-factor because now each bootstrap sample is the same size as the full sample.\n",
    "\n",
    "Important: boostrap and jackknife are generally less (or not at all) sensitive to measures of outliers, e.g. quartiles, although this can be overcome by removing multiple points (subsamples $> 1$). \n",
    "\n",
    "Bootstrapping seems to be moderately preferred by the cognoscenti over subsampling or jackknife, but the fact that involves replacement (and a bootstrap sample therefore has some identical elements) can cause problems in some cases, so think about what you are doing.\n",
    "\n",
    "\n",
    "#### General Remarks\n",
    "\n",
    "All of these approaches are implicitly \"transferring\" the error bars as discussed above.  We are implicitly assuming that any model that would actually fit the data would have a similar distribution and would therefore predict similar errors.\n",
    "\n",
    "\n",
    "The idea behind all three is that the data are drawn from some distrbution and that we can estimate that distribution from the data themselves.  Each subsample (or jackknife sample, or bootstrap sample) is drawn from this distribution, so we get an internal estimate of what variation is expected in data drawn from this distribution.\n",
    "\n",
    "\n",
    "A critical assumption for any of these methods is that the individual subsamples are _independent_.  \n",
    "\n",
    "For the quasar case described above, this assumption is probably fine, since the regions of the universe sampled by different quasars are far enough apart that they are uncorrelated.\n",
    "\n",
    "Suppose we are instead trying to estimate uncertainties in the galaxy correlation function measured from a redshift survey.\n",
    "\n",
    "Each galaxy is a separate data point, but they are highly correlated because they trace the same underlying structure (e.g., the same clusters and superclusters).\n",
    "\n",
    "Using subsamples, jackknife, or bootstrap with individual galaxies would severely underestimate the errors.\n",
    "\n",
    "For this case, we need to define subsamples that are spatially contiguous volumes, large enough that the estimates of the correlation function in each subsample are independent.  Roughly speaking, we want to be sure that the spatial size of each subsample is large compared to the largest coherent structures that are found in the universe.\n",
    "\n",
    "For an example of this approach, see Zehavi et al. 2005, ApJ, 630, 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error bars from artificial data sets \n",
    "\n",
    "If we have a model that we want to test, and we can generate complete, independent artificial data sets from the model, then it is better to estimate errors and covariances from large numbers of mock data sets instead of using these \"internal\" techniques to estimate errors and covariances.\n",
    "\n",
    "For example, it is now common practice to create artificial mock galaxy catalogs from cosmological simulations to estimate errors and covariances for galaxy clustering measurements.\n",
    "\n",
    "Yet this can be very computationally demanding, and developing  efficient tools for creating simulated data sets that are \"accurate enough\" for evaluating errors can be a research problem in itself.\n",
    "\n",
    "In principle one should generate different sets of mock catalogs for all models being tested, or evaluate the dependence of the covariance matrix on model parameters.\n",
    "\n",
    "The accuracy required to estimate errors is usually lower than the accuracy required to evaluate parameters by fitting a model to the data.\n",
    "\n",
    "In practice this approach is usually applied for a fiducial model that is expected to represent the properties of the data reasonably well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise and bias in covariance matrices\n",
    "\n",
    "Sample Covariance Matrix\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{N-1} \\sum^N (\\bar{x}_i - \\bar{x}) (bar{x}_j - \\bar{x})^T\n",
    "$$\n",
    "with $N$ data points, the model has $k$ parameters, and $C$ is a $k \\times k$ matrix. \n",
    "\n",
    "Whether one is using an internal method or artificial data sets, one should be aware of the potential problem of noise in the estimated covariance matrix, since one may be estimating large numbers of $\\sigma_{ij}$.\n",
    "\n",
    "Even if the individual estimates are unbiased, noise may cause some of them to be artificially large.  Since it is the inverse of the covariance matrix that gets used in evaluating the likelihood, noisy estimates of the covariance matrix can cause misleading conclusions about best-fit parameter values, parameter uncertainties, or relative merit of models.\n",
    "\n",
    "If you have an idea of what the general structure of the covariance matrix should be, you can impose \"regularization\" constraints to reduce noise.  See  Padmanabhan et al. (2016, MNRAS 460, 1567, arXiv:1512.01241) for a recent discussion and references therein.\n",
    "\n",
    "A somewhat separate problem is that the inverse covariance matrix estimated from a finite number of artificial data sets or subsamples can be systematically biased.  This problem (and a partial solution) is discussed by Hartlap \\& Schneider (2007, A\\&A 464, 399) and more recently by Paz \\& Sanchez (2015, MNRAS 454, 4326)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosmology Example\n",
    "\n",
    "Imagine you have some dataset $\\bar{D}$ that are measurements, for example of the power spectrum or the 2PCF. It is common to assume the data are drawn from multivariate Gaussians with mean $<\\bar{D}>$ and the covariance matrix. \n",
    "\n",
    "Take a moddel with parameters $vec \\theta$ and $T(\\theta) = <D> (\\vec \\theta)$ so theta\n",
    "$$ \n",
    "\\mathcal{L} (\\bar{D} | \\theta\\,\\psi) = |\\psi|^{1/2} \\exp{\\frac{1}{2} \\chi2 (\\bar{D} \\bar{\\theta}, \\psi) }\n",
    "$$\n",
    "The inverse of the covariance is sometimes called the precision matrix, which is $\\psi = C^{-1}$. \n",
    "\n",
    "In addition $chi^2 = \\sum{i,i} (D_i - T_i \\theta) \\psi \\sum{i,i} (D_i - T_i \\theta)$\n",
    "\n",
    "One needs to know $\\psi$ to evaluate $\\chi^2$ and $\\mathcal{L}$. It is common to derive $N_s$ from synthetic measurements $\\bar{D}^k$. The covariance of the sample is:\n",
    "\n",
    "$$\n",
    "\\hat{C}_{ij} = \\frac{1}{N_s - 1} \\sum^{N_s}_k (D^k_i - \\bar{D}_i) (D^k_j - \\bar{D}_j)\n",
    "$$\n",
    "\n",
    "where $\\bar{D}_i = \\frac{1}{N_s} \\sum_k D^k_i$ is an unbiased, ensemble average of $D$. \n",
    "\n",
    "With in-depth realizations, the uncertainties for $C$ and $\\psi$ follow a Wishhart distribution, and the inverse is asymmetric so that $\\hat{C}^{-1}$ is bais-corrected with \n",
    "$$\n",
    "\\hat{\\psi} = \\left( 1 + \\frac{N_b + 1}{N_s + 1} \\right) \\hat{C}^{-1}\n",
    "$$\n",
    "for $N_b$ bins of $\\hat{D}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
